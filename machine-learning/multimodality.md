

### LLaVA

> https://llava-vl.github.io
> https://github.com/haotian-liu/LLaVA
> https://huggingface.co/liuhaotian

LLaVa connects pre-trained [CLIP ViT-L/14](https://openai.com/research/clip) visual encoder and large language model [Vicuna](https://github.com/lm-sys/FastChat), using a simple projection matrix. LLaVA-1.5 uses Llama 2.

